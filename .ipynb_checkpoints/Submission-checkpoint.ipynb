{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup\n",
    "=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas\n",
    "from sklearn import svm, metrics, linear_model\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import feature_selection as fs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from math import pow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#instances x #features(+1) = (4850, 1583)\n"
     ]
    }
   ],
   "source": [
    "# load basic feature data\n",
    "data, meta = arff.loadarff('emobase2010.arff')\n",
    "df = pandas.DataFrame.from_records(data)\n",
    "print('#instances x #features(+1) =', df.shape)\n",
    "\n",
    "# add ids from separate file to dataframe\n",
    "with open('instances.txt', 'r') as id_file:\n",
    "    instances = id_file.readlines()\n",
    "    fids = [instance.split('.')[0] \n",
    "            for instance in instances]\n",
    "    uids = [instance.split('.')[1].replace('\\n', '') \n",
    "            for instance in instances]\n",
    "    df['fid'] = fids\n",
    "    df['uid'] = uids\n",
    "cols = list(data.dtype.names)\n",
    "cols.append('fid')\n",
    "cols.append('uid')\n",
    "df.columns = cols\n",
    "\n",
    "# load emotion ratings\n",
    "with open('eval/intensity.txt', 'r') as csv_file:\n",
    "    df2 = pandas.read_csv(csv_file, dtype={'uid':'str'})\n",
    "\n",
    "# compute mean intensity rating for each instance...\n",
    "df2['mean'] = df2[['EF01','EF02','EF03','EF04','EF06',\n",
    "                   'EM01','EM02','EM03','EM04','EM05',\n",
    "                   'EM06','EM07','EM08','EM09','EM11',\n",
    "                   'EM12','EM14','EM15']].mean(axis=1)\n",
    "# ...and drop individual ratings\n",
    "df2 = df2.drop(['EF01','EF02','EF03','EF04','EF06',\n",
    "                'EM01','EM02','EM03','EM04','EM05',\n",
    "                'EM06','EM07','EM08','EM09','EM11',\n",
    "                'EM12','EM14','EM15'], axis=1)\n",
    "\n",
    "# merge features with emotion ratings (inner join)\n",
    "# (different order -> need to use merge)\n",
    "df = pandas.merge(df, df2, on=['fid', 'uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUR    386\n",
      "DIS    223\n",
      "JOY    215\n",
      "ANT    191\n",
      "SAD    170\n",
      "ACC    164\n",
      "ANG    146\n",
      "FEA     87\n",
      "Name: emotion, dtype: int64\n",
      "sum: 1582\n"
     ]
    }
   ],
   "source": [
    "# remove instances with 'neutral', 'unknown' and 'other' label\n",
    "a = df['emotion']!='NEU'\n",
    "b = df['emotion']!='UNK'\n",
    "c = df['emotion']!='OTH'\n",
    "df = df.loc[a&b&c]\n",
    "\n",
    "# number of instances per class\n",
    "print(df['emotion'].value_counts())\n",
    "\n",
    "# total number of remaining instances\n",
    "print('sum:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pcm_loudness_sma_maxPos</th>\n",
       "      <th>pcm_loudness_sma_minPos</th>\n",
       "      <th>pcm_loudness_sma_amean</th>\n",
       "      <th>pcm_loudness_sma_linregc1</th>\n",
       "      <th>pcm_loudness_sma_linregc2</th>\n",
       "      <th>pcm_loudness_sma_linregerrA</th>\n",
       "      <th>pcm_loudness_sma_linregerrQ</th>\n",
       "      <th>pcm_loudness_sma_stddev</th>\n",
       "      <th>pcm_loudness_sma_skewness</th>\n",
       "      <th>pcm_loudness_sma_kurtosis</th>\n",
       "      <th>...</th>\n",
       "      <th>shimmerLocal_sma_de_percentile99.0</th>\n",
       "      <th>shimmerLocal_sma_de_upleveltime75</th>\n",
       "      <th>shimmerLocal_sma_de_upleveltime90</th>\n",
       "      <th>F0final__Turn_numOnsets</th>\n",
       "      <th>F0final__Turn_duration</th>\n",
       "      <th>class</th>\n",
       "      <th>fid</th>\n",
       "      <th>uid</th>\n",
       "      <th>emotion</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258640</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.239240</td>\n",
       "      <td>0.078132</td>\n",
       "      <td>0.008620</td>\n",
       "      <td>0.093542</td>\n",
       "      <td>-0.021157</td>\n",
       "      <td>2.429211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030707</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.57</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>01_MMK_2</td>\n",
       "      <td>013</td>\n",
       "      <td>SUR</td>\n",
       "      <td>2.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.151027</td>\n",
       "      <td>-0.002178</td>\n",
       "      <td>0.215283</td>\n",
       "      <td>0.116132</td>\n",
       "      <td>0.019550</td>\n",
       "      <td>0.144819</td>\n",
       "      <td>0.891886</td>\n",
       "      <td>2.575608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206826</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>05_MYH</td>\n",
       "      <td>017</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.132076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298068</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.297868</td>\n",
       "      <td>0.096203</td>\n",
       "      <td>0.013251</td>\n",
       "      <td>0.115113</td>\n",
       "      <td>0.004746</td>\n",
       "      <td>2.158694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>b'FEA'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>396</td>\n",
       "      <td>JOY</td>\n",
       "      <td>2.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082860</td>\n",
       "      <td>0.003008</td>\n",
       "      <td>0.063309</td>\n",
       "      <td>0.017077</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>-0.266671</td>\n",
       "      <td>2.478187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>430</td>\n",
       "      <td>SUR</td>\n",
       "      <td>1.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.079869</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.066220</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.035386</td>\n",
       "      <td>-0.056963</td>\n",
       "      <td>1.861217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>b'SAD'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>397</td>\n",
       "      <td>SAD</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409581</td>\n",
       "      <td>0.016374</td>\n",
       "      <td>0.278593</td>\n",
       "      <td>0.115249</td>\n",
       "      <td>0.019151</td>\n",
       "      <td>0.159953</td>\n",
       "      <td>0.224859</td>\n",
       "      <td>2.499954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157520</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>b'ANG'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>081</td>\n",
       "      <td>ANG</td>\n",
       "      <td>2.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176277</td>\n",
       "      <td>0.011218</td>\n",
       "      <td>0.086531</td>\n",
       "      <td>0.045721</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.075714</td>\n",
       "      <td>0.269838</td>\n",
       "      <td>2.357958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.19</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>04_MNN</td>\n",
       "      <td>703</td>\n",
       "      <td>SUR</td>\n",
       "      <td>1.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.858823</td>\n",
       "      <td>0.336360</td>\n",
       "      <td>-0.004572</td>\n",
       "      <td>0.528398</td>\n",
       "      <td>0.159192</td>\n",
       "      <td>0.036350</td>\n",
       "      <td>0.221215</td>\n",
       "      <td>0.911612</td>\n",
       "      <td>4.025831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195774</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.87</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>042</td>\n",
       "      <td>ANT</td>\n",
       "      <td>3.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.204698</td>\n",
       "      <td>0.412752</td>\n",
       "      <td>0.147276</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>0.188270</td>\n",
       "      <td>0.090380</td>\n",
       "      <td>0.013479</td>\n",
       "      <td>0.118502</td>\n",
       "      <td>1.222397</td>\n",
       "      <td>4.326225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182839</td>\n",
       "      <td>0.025907</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>569</td>\n",
       "      <td>ANT</td>\n",
       "      <td>2.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.631206</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.233684</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.191119</td>\n",
       "      <td>0.140190</td>\n",
       "      <td>0.028470</td>\n",
       "      <td>0.170537</td>\n",
       "      <td>0.779474</td>\n",
       "      <td>3.093070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137699</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.43</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>109</td>\n",
       "      <td>JOY</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.357001</td>\n",
       "      <td>0.157387</td>\n",
       "      <td>0.042227</td>\n",
       "      <td>0.038617</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.226584</td>\n",
       "      <td>-0.218739</td>\n",
       "      <td>1.422887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017937</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>093</td>\n",
       "      <td>SUR</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375652</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>0.487617</td>\n",
       "      <td>0.088172</td>\n",
       "      <td>0.015544</td>\n",
       "      <td>0.140894</td>\n",
       "      <td>-0.079412</td>\n",
       "      <td>2.362413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048705</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>355</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.275318</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.253402</td>\n",
       "      <td>0.116455</td>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.136449</td>\n",
       "      <td>0.640075</td>\n",
       "      <td>2.055694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>b'DIS'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>880</td>\n",
       "      <td>DIS</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219314</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.053501</td>\n",
       "      <td>0.122690</td>\n",
       "      <td>0.023863</td>\n",
       "      <td>0.182216</td>\n",
       "      <td>1.158418</td>\n",
       "      <td>3.588311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095232</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>b'DIS'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>507</td>\n",
       "      <td>DIS</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.269504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398444</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.353523</td>\n",
       "      <td>0.141695</td>\n",
       "      <td>0.032923</td>\n",
       "      <td>0.183317</td>\n",
       "      <td>-0.088399</td>\n",
       "      <td>2.815467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174237</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.43</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>378</td>\n",
       "      <td>ANT</td>\n",
       "      <td>3.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701899</td>\n",
       "      <td>0.166079</td>\n",
       "      <td>0.120623</td>\n",
       "      <td>0.173790</td>\n",
       "      <td>0.041734</td>\n",
       "      <td>0.431903</td>\n",
       "      <td>-0.587526</td>\n",
       "      <td>1.675794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084998</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>166</td>\n",
       "      <td>SUR</td>\n",
       "      <td>3.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.300695</td>\n",
       "      <td>-0.004489</td>\n",
       "      <td>0.460045</td>\n",
       "      <td>0.255184</td>\n",
       "      <td>0.099935</td>\n",
       "      <td>0.329602</td>\n",
       "      <td>1.614156</td>\n",
       "      <td>5.444261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091322</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>05_MYH</td>\n",
       "      <td>224</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404171</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.150621</td>\n",
       "      <td>0.130672</td>\n",
       "      <td>0.028538</td>\n",
       "      <td>0.224844</td>\n",
       "      <td>0.140883</td>\n",
       "      <td>2.188507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131123</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>b'SAD'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>220</td>\n",
       "      <td>SAD</td>\n",
       "      <td>3.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.899083</td>\n",
       "      <td>0.393751</td>\n",
       "      <td>-0.001985</td>\n",
       "      <td>0.500914</td>\n",
       "      <td>0.167882</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.214550</td>\n",
       "      <td>0.615264</td>\n",
       "      <td>2.596484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114048</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.089888</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>b'OTH'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>603</td>\n",
       "      <td>ANT</td>\n",
       "      <td>3.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>0.260437</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.260810</td>\n",
       "      <td>0.144721</td>\n",
       "      <td>0.035285</td>\n",
       "      <td>0.187842</td>\n",
       "      <td>1.044415</td>\n",
       "      <td>3.459790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130152</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>799</td>\n",
       "      <td>SUR</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.345361</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.319425</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.319915</td>\n",
       "      <td>0.186396</td>\n",
       "      <td>0.055439</td>\n",
       "      <td>0.235455</td>\n",
       "      <td>0.902609</td>\n",
       "      <td>3.480595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144395</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.96</td>\n",
       "      <td>b'NEU'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>748</td>\n",
       "      <td>ANT</td>\n",
       "      <td>3.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.095789</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.037956</td>\n",
       "      <td>0.030712</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.055109</td>\n",
       "      <td>1.277884</td>\n",
       "      <td>3.829860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026227</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>b'DIS'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>515</td>\n",
       "      <td>DIS</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225953</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.179879</td>\n",
       "      <td>0.049601</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.066010</td>\n",
       "      <td>-0.982870</td>\n",
       "      <td>4.645099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019787</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>04_MNN</td>\n",
       "      <td>530</td>\n",
       "      <td>SUR</td>\n",
       "      <td>1.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.295636</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.248218</td>\n",
       "      <td>0.071733</td>\n",
       "      <td>0.012656</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>1.485007</td>\n",
       "      <td>5.577529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021571</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>b'SAD'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>137</td>\n",
       "      <td>SAD</td>\n",
       "      <td>2.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.131770</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.066053</td>\n",
       "      <td>0.060436</td>\n",
       "      <td>0.005751</td>\n",
       "      <td>0.085082</td>\n",
       "      <td>0.772183</td>\n",
       "      <td>3.466579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145755</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>b'ANG'</td>\n",
       "      <td>01_MAD_2</td>\n",
       "      <td>136</td>\n",
       "      <td>ANG</td>\n",
       "      <td>2.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448537</td>\n",
       "      <td>-0.012413</td>\n",
       "      <td>0.616112</td>\n",
       "      <td>0.134064</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.204533</td>\n",
       "      <td>0.261131</td>\n",
       "      <td>1.844050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039671</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>329</td>\n",
       "      <td>SUR</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128672</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.116572</td>\n",
       "      <td>0.026929</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.032908</td>\n",
       "      <td>0.107640</td>\n",
       "      <td>1.883158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033469</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>174</td>\n",
       "      <td>DIS</td>\n",
       "      <td>2.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>0.228356</td>\n",
       "      <td>-0.002959</td>\n",
       "      <td>0.323035</td>\n",
       "      <td>0.090099</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>0.134537</td>\n",
       "      <td>1.469908</td>\n",
       "      <td>5.461781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170124</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>01_MMK_2</td>\n",
       "      <td>068</td>\n",
       "      <td>ACC</td>\n",
       "      <td>3.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.202765</td>\n",
       "      <td>0.866359</td>\n",
       "      <td>0.237512</td>\n",
       "      <td>-0.001004</td>\n",
       "      <td>0.345970</td>\n",
       "      <td>0.121293</td>\n",
       "      <td>0.025674</td>\n",
       "      <td>0.172136</td>\n",
       "      <td>1.521258</td>\n",
       "      <td>6.536967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218582</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.19</td>\n",
       "      <td>b'DIS'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>617</td>\n",
       "      <td>DIS</td>\n",
       "      <td>3.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.409978</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.388882</td>\n",
       "      <td>0.229495</td>\n",
       "      <td>0.075494</td>\n",
       "      <td>0.275034</td>\n",
       "      <td>0.308803</td>\n",
       "      <td>2.558224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205201</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.006173</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.84</td>\n",
       "      <td>b'FEA'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>161</td>\n",
       "      <td>FEA</td>\n",
       "      <td>3.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.261215</td>\n",
       "      <td>0.008779</td>\n",
       "      <td>0.212934</td>\n",
       "      <td>0.063037</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>0.081765</td>\n",
       "      <td>0.201231</td>\n",
       "      <td>2.083458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125382</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>b'ANG'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>284</td>\n",
       "      <td>DIS</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146765</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.104213</td>\n",
       "      <td>0.077545</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.093428</td>\n",
       "      <td>0.813886</td>\n",
       "      <td>3.065942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236800</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>b'FEA'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>560</td>\n",
       "      <td>FEA</td>\n",
       "      <td>3.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296900</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.269669</td>\n",
       "      <td>0.142189</td>\n",
       "      <td>0.029106</td>\n",
       "      <td>0.171364</td>\n",
       "      <td>0.756017</td>\n",
       "      <td>2.545121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257890</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>b'FEA'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>522</td>\n",
       "      <td>FEA</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365368</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.124122</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.159980</td>\n",
       "      <td>0.522443</td>\n",
       "      <td>2.863939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165241</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>254</td>\n",
       "      <td>SUR</td>\n",
       "      <td>3.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.199634</td>\n",
       "      <td>0.005026</td>\n",
       "      <td>0.079012</td>\n",
       "      <td>0.083382</td>\n",
       "      <td>0.010602</td>\n",
       "      <td>0.125114</td>\n",
       "      <td>0.414126</td>\n",
       "      <td>1.937547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125597</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>04_MNN</td>\n",
       "      <td>611</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.274116</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.231173</td>\n",
       "      <td>0.100881</td>\n",
       "      <td>0.016131</td>\n",
       "      <td>0.129449</td>\n",
       "      <td>0.618585</td>\n",
       "      <td>3.666859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108104</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.14</td>\n",
       "      <td>b'ANG'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>193</td>\n",
       "      <td>ANG</td>\n",
       "      <td>2.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0.647887</td>\n",
       "      <td>0.183099</td>\n",
       "      <td>0.250265</td>\n",
       "      <td>0.002530</td>\n",
       "      <td>0.161707</td>\n",
       "      <td>0.060057</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.093145</td>\n",
       "      <td>-0.249976</td>\n",
       "      <td>2.544119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134254</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>b'FEA'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>216</td>\n",
       "      <td>FEA</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.416961</td>\n",
       "      <td>0.259691</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>0.375605</td>\n",
       "      <td>0.148721</td>\n",
       "      <td>0.033314</td>\n",
       "      <td>0.194486</td>\n",
       "      <td>0.932535</td>\n",
       "      <td>3.531015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162085</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>04_MNN</td>\n",
       "      <td>488</td>\n",
       "      <td>ANT</td>\n",
       "      <td>3.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.489822</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.241268</td>\n",
       "      <td>0.171497</td>\n",
       "      <td>0.035085</td>\n",
       "      <td>0.238853</td>\n",
       "      <td>-0.625360</td>\n",
       "      <td>1.838713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109535</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>186</td>\n",
       "      <td>FEA</td>\n",
       "      <td>2.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347809</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.302857</td>\n",
       "      <td>0.071328</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.090693</td>\n",
       "      <td>-0.987001</td>\n",
       "      <td>4.310684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021577</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>240</td>\n",
       "      <td>ACC</td>\n",
       "      <td>3.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.507787</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.241833</td>\n",
       "      <td>0.083405</td>\n",
       "      <td>0.288839</td>\n",
       "      <td>0.481356</td>\n",
       "      <td>2.405025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162363</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.016529</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>b'ANG'</td>\n",
       "      <td>05_MYH</td>\n",
       "      <td>102</td>\n",
       "      <td>ANG</td>\n",
       "      <td>2.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224595</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>0.096245</td>\n",
       "      <td>0.096044</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>0.138679</td>\n",
       "      <td>0.552107</td>\n",
       "      <td>2.599739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165817</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>779</td>\n",
       "      <td>SUR</td>\n",
       "      <td>2.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>0.433121</td>\n",
       "      <td>0.579618</td>\n",
       "      <td>0.285819</td>\n",
       "      <td>-0.000343</td>\n",
       "      <td>0.312593</td>\n",
       "      <td>0.111697</td>\n",
       "      <td>0.018506</td>\n",
       "      <td>0.136925</td>\n",
       "      <td>-0.173052</td>\n",
       "      <td>2.190243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156104</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>b'DIS'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>106</td>\n",
       "      <td>SAD</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455406</td>\n",
       "      <td>0.170026</td>\n",
       "      <td>0.200367</td>\n",
       "      <td>0.042595</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.196114</td>\n",
       "      <td>-0.113575</td>\n",
       "      <td>1.979908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032495</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>282</td>\n",
       "      <td>SUR</td>\n",
       "      <td>2.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.686567</td>\n",
       "      <td>0.338919</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.232166</td>\n",
       "      <td>0.266527</td>\n",
       "      <td>0.091394</td>\n",
       "      <td>0.308720</td>\n",
       "      <td>0.651829</td>\n",
       "      <td>2.432021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160502</td>\n",
       "      <td>0.096154</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>01_MAD_2</td>\n",
       "      <td>107</td>\n",
       "      <td>ANT</td>\n",
       "      <td>2.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>0.102804</td>\n",
       "      <td>0.724299</td>\n",
       "      <td>0.395546</td>\n",
       "      <td>-0.001596</td>\n",
       "      <td>0.565521</td>\n",
       "      <td>0.171971</td>\n",
       "      <td>0.047479</td>\n",
       "      <td>0.239165</td>\n",
       "      <td>0.536787</td>\n",
       "      <td>2.921299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164699</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.005587</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.16</td>\n",
       "      <td>b'JOY'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>525</td>\n",
       "      <td>JOY</td>\n",
       "      <td>4.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.206940</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.043572</td>\n",
       "      <td>0.090479</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>0.140872</td>\n",
       "      <td>-0.092935</td>\n",
       "      <td>1.525592</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175008</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>b'SAD'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>211</td>\n",
       "      <td>SAD</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.280923</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>0.385938</td>\n",
       "      <td>0.078791</td>\n",
       "      <td>0.010491</td>\n",
       "      <td>0.119460</td>\n",
       "      <td>0.748059</td>\n",
       "      <td>2.748431</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040864</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>b'JOY'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>377</td>\n",
       "      <td>JOY</td>\n",
       "      <td>2.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.151245</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>0.274210</td>\n",
       "      <td>0.057452</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.112323</td>\n",
       "      <td>1.140674</td>\n",
       "      <td>3.641949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103205</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>b'SAD'</td>\n",
       "      <td>04_MNN</td>\n",
       "      <td>095</td>\n",
       "      <td>SAD</td>\n",
       "      <td>2.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.195801</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.228314</td>\n",
       "      <td>0.089730</td>\n",
       "      <td>0.013432</td>\n",
       "      <td>0.117428</td>\n",
       "      <td>1.043265</td>\n",
       "      <td>4.509830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.56</td>\n",
       "      <td>b'SAD'</td>\n",
       "      <td>04_MNN</td>\n",
       "      <td>732</td>\n",
       "      <td>SAD</td>\n",
       "      <td>2.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>0.769911</td>\n",
       "      <td>0.353982</td>\n",
       "      <td>0.242305</td>\n",
       "      <td>0.001544</td>\n",
       "      <td>0.155814</td>\n",
       "      <td>0.125442</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.160707</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>2.703706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158598</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.15</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>04_MNN</td>\n",
       "      <td>353</td>\n",
       "      <td>JOY</td>\n",
       "      <td>2.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483672</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>0.518234</td>\n",
       "      <td>0.184479</td>\n",
       "      <td>0.057761</td>\n",
       "      <td>0.241179</td>\n",
       "      <td>0.828856</td>\n",
       "      <td>4.122412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104117</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>03_FTY</td>\n",
       "      <td>022</td>\n",
       "      <td>JOY</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>0.282828</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.193433</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>0.220638</td>\n",
       "      <td>0.086929</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.109994</td>\n",
       "      <td>0.835241</td>\n",
       "      <td>3.599408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200632</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>b'ACC'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>138</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490987</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>0.590728</td>\n",
       "      <td>0.245369</td>\n",
       "      <td>0.088717</td>\n",
       "      <td>0.303484</td>\n",
       "      <td>0.463498</td>\n",
       "      <td>2.289197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075362</td>\n",
       "      <td>0.087912</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>b'SUR'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>705</td>\n",
       "      <td>SUR</td>\n",
       "      <td>3.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.471870</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.525053</td>\n",
       "      <td>0.220047</td>\n",
       "      <td>0.076685</td>\n",
       "      <td>0.278662</td>\n",
       "      <td>0.735594</td>\n",
       "      <td>3.157729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137919</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>b'JOY'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>559</td>\n",
       "      <td>JOY</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.402597</td>\n",
       "      <td>0.195023</td>\n",
       "      <td>-0.001029</td>\n",
       "      <td>0.273742</td>\n",
       "      <td>0.080968</td>\n",
       "      <td>0.011566</td>\n",
       "      <td>0.116868</td>\n",
       "      <td>1.391975</td>\n",
       "      <td>5.639768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068326</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.56</td>\n",
       "      <td>b'SAD'</td>\n",
       "      <td>01_MAD_1</td>\n",
       "      <td>139</td>\n",
       "      <td>SAD</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>0.639535</td>\n",
       "      <td>0.889535</td>\n",
       "      <td>0.448336</td>\n",
       "      <td>-0.000592</td>\n",
       "      <td>0.498986</td>\n",
       "      <td>0.222581</td>\n",
       "      <td>0.069335</td>\n",
       "      <td>0.264953</td>\n",
       "      <td>0.505777</td>\n",
       "      <td>2.538045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209278</td>\n",
       "      <td>0.104575</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.74</td>\n",
       "      <td>b'ANG'</td>\n",
       "      <td>05_MYH</td>\n",
       "      <td>269</td>\n",
       "      <td>ANG</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>0.226744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279004</td>\n",
       "      <td>-0.000409</td>\n",
       "      <td>0.313983</td>\n",
       "      <td>0.119726</td>\n",
       "      <td>0.020616</td>\n",
       "      <td>0.145014</td>\n",
       "      <td>0.587836</td>\n",
       "      <td>3.115611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205492</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.74</td>\n",
       "      <td>b'OTH'</td>\n",
       "      <td>01_MMK_1</td>\n",
       "      <td>588</td>\n",
       "      <td>JOY</td>\n",
       "      <td>2.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319201</td>\n",
       "      <td>-0.001090</td>\n",
       "      <td>0.363874</td>\n",
       "      <td>0.146909</td>\n",
       "      <td>0.031294</td>\n",
       "      <td>0.178816</td>\n",
       "      <td>0.438240</td>\n",
       "      <td>2.250966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061972</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>b'ANG'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>412</td>\n",
       "      <td>ANG</td>\n",
       "      <td>2.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>0.593220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311989</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.247355</td>\n",
       "      <td>0.108990</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.131413</td>\n",
       "      <td>0.230096</td>\n",
       "      <td>2.246752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187651</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>b'ANT'</td>\n",
       "      <td>02_MTN</td>\n",
       "      <td>860</td>\n",
       "      <td>SUR</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1582 rows × 1587 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pcm_loudness_sma_maxPos  pcm_loudness_sma_minPos  \\\n",
       "0                    0.290909                 0.000000   \n",
       "1                    0.533333                 0.883333   \n",
       "2                    0.132076                 0.000000   \n",
       "3                    0.785714                 0.000000   \n",
       "4                    0.592593                 0.407407   \n",
       "5                    0.764706                 0.000000   \n",
       "6                    0.823529                 0.000000   \n",
       "7                    0.117647                 0.858823   \n",
       "8                    0.204698                 0.412752   \n",
       "9                    0.631206                 0.978723   \n",
       "10                   0.800000                 0.000000   \n",
       "11                   0.272727                 0.000000   \n",
       "12                   0.307692                 0.076923   \n",
       "13                   0.773585                 0.000000   \n",
       "14                   0.269504                 0.000000   \n",
       "15                   0.500000                 0.000000   \n",
       "16                   0.097222                 0.486111   \n",
       "17                   0.500000                 0.000000   \n",
       "18                   0.146789                 0.899083   \n",
       "19                   0.587629                 0.680412   \n",
       "20                   0.345361                 0.948454   \n",
       "21                   0.800000                 0.600000   \n",
       "22                   0.600000                 0.000000   \n",
       "23                   0.452381                 0.000000   \n",
       "24                   0.819672                 0.524590   \n",
       "25                   0.285714                 0.000000   \n",
       "26                   0.733333                 0.000000   \n",
       "27                   0.184615                 0.784615   \n",
       "28                   0.202765                 0.866359   \n",
       "29                   0.263736                 0.142857   \n",
       "...                       ...                      ...   \n",
       "1552                 0.750000                 0.416667   \n",
       "1553                 0.971831                 0.000000   \n",
       "1554                 0.589744                 0.000000   \n",
       "1555                 0.200000                 0.000000   \n",
       "1556                 0.734694                 0.469388   \n",
       "1557                 0.312500                 0.187500   \n",
       "1558                 0.647887                 0.183099   \n",
       "1559                 0.021201                 0.416961   \n",
       "1560                 0.322581                 0.225806   \n",
       "1561                 0.380952                 0.000000   \n",
       "1562                 0.623188                 0.405797   \n",
       "1563                 0.675000                 0.000000   \n",
       "1564                 0.433121                 0.579618   \n",
       "1565                 0.750000                 0.000000   \n",
       "1566                 0.925373                 0.686567   \n",
       "1567                 0.102804                 0.724299   \n",
       "1568                 0.796610                 0.220339   \n",
       "1569                 0.333333                 0.986111   \n",
       "1570                 0.127660                 0.957447   \n",
       "1571                 0.545455                 0.090909   \n",
       "1572                 0.769911                 0.353982   \n",
       "1573                 0.120000                 0.000000   \n",
       "1574                 0.282828                 0.565657   \n",
       "1575                 0.156250                 0.000000   \n",
       "1576                 0.506494                 0.922078   \n",
       "1577                 0.285714                 0.402597   \n",
       "1578                 0.639535                 0.889535   \n",
       "1579                 0.226744                 0.000000   \n",
       "1580                 0.445783                 0.000000   \n",
       "1581                 0.593220                 0.000000   \n",
       "\n",
       "      pcm_loudness_sma_amean  pcm_loudness_sma_linregc1  \\\n",
       "0                   0.258640                   0.000719   \n",
       "1                   0.151027                  -0.002178   \n",
       "2                   0.298068                   0.000008   \n",
       "3                   0.082860                   0.003008   \n",
       "4                   0.079869                   0.001050   \n",
       "5                   0.409581                   0.016374   \n",
       "6                   0.176277                   0.011218   \n",
       "7                   0.336360                  -0.004572   \n",
       "8                   0.147276                  -0.000276   \n",
       "9                   0.233684                   0.000608   \n",
       "10                  0.357001                   0.157387   \n",
       "11                  0.375652                  -0.003445   \n",
       "12                  0.275318                   0.003653   \n",
       "13                  0.219314                   0.003158   \n",
       "14                  0.398444                   0.000642   \n",
       "15                  0.701899                   0.166079   \n",
       "16                  0.300695                  -0.004489   \n",
       "17                  0.404171                   0.006947   \n",
       "18                  0.393751                  -0.001985   \n",
       "19                  0.260437                  -0.000008   \n",
       "20                  0.319425                  -0.000005   \n",
       "21                  0.095789                   0.008262   \n",
       "22                  0.225953                   0.006582   \n",
       "23                  0.295636                   0.002313   \n",
       "24                  0.131770                   0.002191   \n",
       "25                  0.448537                  -0.012413   \n",
       "26                  0.128672                   0.001729   \n",
       "27                  0.228356                  -0.002959   \n",
       "28                  0.237512                  -0.001004   \n",
       "29                  0.409978                   0.000233   \n",
       "...                      ...                        ...   \n",
       "1552                0.261215                   0.008779   \n",
       "1553                0.146765                   0.001216   \n",
       "1554                0.296900                   0.001433   \n",
       "1555                0.365368                   0.001861   \n",
       "1556                0.199634                   0.005026   \n",
       "1557                0.274116                   0.000774   \n",
       "1558                0.250265                   0.002530   \n",
       "1559                0.259691                  -0.000822   \n",
       "1560                0.489822                   0.016570   \n",
       "1561                0.347809                   0.004495   \n",
       "1562                0.507787                  -0.000120   \n",
       "1563                0.224595                   0.006582   \n",
       "1564                0.285819                  -0.000343   \n",
       "1565                0.455406                   0.170026   \n",
       "1566                0.338919                   0.003235   \n",
       "1567                0.395546                  -0.001596   \n",
       "1568                0.206940                   0.005633   \n",
       "1569                0.280923                  -0.002958   \n",
       "1570                0.151245                  -0.005346   \n",
       "1571                0.195801                  -0.000425   \n",
       "1572                0.242305                   0.001544   \n",
       "1573                0.483672                  -0.000698   \n",
       "1574                0.193433                  -0.000276   \n",
       "1575                0.490987                  -0.002100   \n",
       "1576                0.471870                  -0.001400   \n",
       "1577                0.195023                  -0.001029   \n",
       "1578                0.448336                  -0.000592   \n",
       "1579                0.279004                  -0.000409   \n",
       "1580                0.319201                  -0.001090   \n",
       "1581                0.311989                   0.002229   \n",
       "\n",
       "      pcm_loudness_sma_linregc2  pcm_loudness_sma_linregerrA  \\\n",
       "0                      0.239240                     0.078132   \n",
       "1                      0.215283                     0.116132   \n",
       "2                      0.297868                     0.096203   \n",
       "3                      0.063309                     0.017077   \n",
       "4                      0.066220                     0.028754   \n",
       "5                      0.278593                     0.115249   \n",
       "6                      0.086531                     0.045721   \n",
       "7                      0.528398                     0.159192   \n",
       "8                      0.188270                     0.090380   \n",
       "9                      0.191119                     0.140190   \n",
       "10                     0.042227                     0.038617   \n",
       "11                     0.487617                     0.088172   \n",
       "12                     0.253402                     0.116455   \n",
       "13                     0.053501                     0.122690   \n",
       "14                     0.353523                     0.141695   \n",
       "15                     0.120623                     0.173790   \n",
       "16                     0.460045                     0.255184   \n",
       "17                     0.150621                     0.130672   \n",
       "18                     0.500914                     0.167882   \n",
       "19                     0.260810                     0.144721   \n",
       "20                     0.319915                     0.186396   \n",
       "21                     0.037956                     0.030712   \n",
       "22                     0.179879                     0.049601   \n",
       "23                     0.248218                     0.071733   \n",
       "24                     0.066053                     0.060436   \n",
       "25                     0.616112                     0.134064   \n",
       "26                     0.116572                     0.026929   \n",
       "27                     0.323035                     0.090099   \n",
       "28                     0.345970                     0.121293   \n",
       "29                     0.388882                     0.229495   \n",
       "...                         ...                          ...   \n",
       "1552                   0.212934                     0.063037   \n",
       "1553                   0.104213                     0.077545   \n",
       "1554                   0.269669                     0.142189   \n",
       "1555                   0.347685                     0.124122   \n",
       "1556                   0.079012                     0.083382   \n",
       "1557                   0.231173                     0.100881   \n",
       "1558                   0.161707                     0.060057   \n",
       "1559                   0.375605                     0.148721   \n",
       "1560                   0.241268                     0.171497   \n",
       "1561                   0.302857                     0.071328   \n",
       "1562                   0.516000                     0.241833   \n",
       "1563                   0.096245                     0.096044   \n",
       "1564                   0.312593                     0.111697   \n",
       "1565                   0.200367                     0.042595   \n",
       "1566                   0.232166                     0.266527   \n",
       "1567                   0.565521                     0.171971   \n",
       "1568                   0.043572                     0.090479   \n",
       "1569                   0.385938                     0.078791   \n",
       "1570                   0.274210                     0.057452   \n",
       "1571                   0.228314                     0.089730   \n",
       "1572                   0.155814                     0.125442   \n",
       "1573                   0.518234                     0.184479   \n",
       "1574                   0.220638                     0.086929   \n",
       "1575                   0.590728                     0.245369   \n",
       "1576                   0.525053                     0.220047   \n",
       "1577                   0.273742                     0.080968   \n",
       "1578                   0.498986                     0.222581   \n",
       "1579                   0.313983                     0.119726   \n",
       "1580                   0.363874                     0.146909   \n",
       "1581                   0.247355                     0.108990   \n",
       "\n",
       "      pcm_loudness_sma_linregerrQ  pcm_loudness_sma_stddev  \\\n",
       "0                        0.008620                 0.093542   \n",
       "1                        0.019550                 0.144819   \n",
       "2                        0.013251                 0.115113   \n",
       "3                        0.000420                 0.023817   \n",
       "4                        0.001185                 0.035386   \n",
       "5                        0.019151                 0.159953   \n",
       "6                        0.002712                 0.075714   \n",
       "7                        0.036350                 0.221215   \n",
       "8                        0.013479                 0.118502   \n",
       "9                        0.028470                 0.170537   \n",
       "10                       0.001799                 0.226584   \n",
       "11                       0.015544                 0.140894   \n",
       "12                       0.018431                 0.136449   \n",
       "13                       0.023863                 0.182216   \n",
       "14                       0.032923                 0.183317   \n",
       "15                       0.041734                 0.431903   \n",
       "16                       0.099935                 0.329602   \n",
       "17                       0.028538                 0.224844   \n",
       "18                       0.042133                 0.214550   \n",
       "19                       0.035285                 0.187842   \n",
       "20                       0.055439                 0.235455   \n",
       "21                       0.001763                 0.055109   \n",
       "22                       0.003549                 0.066010   \n",
       "23                       0.012656                 0.115942   \n",
       "24                       0.005751                 0.085082   \n",
       "25                       0.031780                 0.204533   \n",
       "26                       0.001027                 0.032908   \n",
       "27                       0.015019                 0.134537   \n",
       "28                       0.025674                 0.172136   \n",
       "29                       0.075494                 0.275034   \n",
       "...                           ...                      ...   \n",
       "1552                     0.005767                 0.081765   \n",
       "1553                     0.008108                 0.093428   \n",
       "1554                     0.029106                 0.171364   \n",
       "1555                     0.025478                 0.159980   \n",
       "1556                     0.010602                 0.125114   \n",
       "1557                     0.016131                 0.129449   \n",
       "1558                     0.005987                 0.093145   \n",
       "1559                     0.033314                 0.194486   \n",
       "1560                     0.035085                 0.238853   \n",
       "1561                     0.007484                 0.090693   \n",
       "1562                     0.083405                 0.288839   \n",
       "1563                     0.013459                 0.138679   \n",
       "1564                     0.018506                 0.136925   \n",
       "1565                     0.002325                 0.196114   \n",
       "1566                     0.091394                 0.308720   \n",
       "1567                     0.047479                 0.239165   \n",
       "1568                     0.010642                 0.140872   \n",
       "1569                     0.010491                 0.119460   \n",
       "1570                     0.007357                 0.112323   \n",
       "1571                     0.013432                 0.117428   \n",
       "1572                     0.023289                 0.160707   \n",
       "1573                     0.057761                 0.241179   \n",
       "1574                     0.011850                 0.109994   \n",
       "1575                     0.088717                 0.303484   \n",
       "1576                     0.076685                 0.278662   \n",
       "1577                     0.011566                 0.116868   \n",
       "1578                     0.069335                 0.264953   \n",
       "1579                     0.020616                 0.145014   \n",
       "1580                     0.031294                 0.178816   \n",
       "1581                     0.015829                 0.131413   \n",
       "\n",
       "      pcm_loudness_sma_skewness  pcm_loudness_sma_kurtosis    ...     \\\n",
       "0                     -0.021157                   2.429211    ...      \n",
       "1                      0.891886                   2.575608    ...      \n",
       "2                      0.004746                   2.158694    ...      \n",
       "3                     -0.266671                   2.478187    ...      \n",
       "4                     -0.056963                   1.861217    ...      \n",
       "5                      0.224859                   2.499954    ...      \n",
       "6                      0.269838                   2.357958    ...      \n",
       "7                      0.911612                   4.025831    ...      \n",
       "8                      1.222397                   4.326225    ...      \n",
       "9                      0.779474                   3.093070    ...      \n",
       "10                    -0.218739                   1.422887    ...      \n",
       "11                    -0.079412                   2.362413    ...      \n",
       "12                     0.640075                   2.055694    ...      \n",
       "13                     1.158418                   3.588311    ...      \n",
       "14                    -0.088399                   2.815467    ...      \n",
       "15                    -0.587526                   1.675794    ...      \n",
       "16                     1.614156                   5.444261    ...      \n",
       "17                     0.140883                   2.188507    ...      \n",
       "18                     0.615264                   2.596484    ...      \n",
       "19                     1.044415                   3.459790    ...      \n",
       "20                     0.902609                   3.480595    ...      \n",
       "21                     1.277884                   3.829860    ...      \n",
       "22                    -0.982870                   4.645099    ...      \n",
       "23                     1.485007                   5.577529    ...      \n",
       "24                     0.772183                   3.466579    ...      \n",
       "25                     0.261131                   1.844050    ...      \n",
       "26                     0.107640                   1.883158    ...      \n",
       "27                     1.469908                   5.461781    ...      \n",
       "28                     1.521258                   6.536967    ...      \n",
       "29                     0.308803                   2.558224    ...      \n",
       "...                         ...                        ...    ...      \n",
       "1552                   0.201231                   2.083458    ...      \n",
       "1553                   0.813886                   3.065942    ...      \n",
       "1554                   0.756017                   2.545121    ...      \n",
       "1555                   0.522443                   2.863939    ...      \n",
       "1556                   0.414126                   1.937547    ...      \n",
       "1557                   0.618585                   3.666859    ...      \n",
       "1558                  -0.249976                   2.544119    ...      \n",
       "1559                   0.932535                   3.531015    ...      \n",
       "1560                  -0.625360                   1.838713    ...      \n",
       "1561                  -0.987001                   4.310684    ...      \n",
       "1562                   0.481356                   2.405025    ...      \n",
       "1563                   0.552107                   2.599739    ...      \n",
       "1564                  -0.173052                   2.190243    ...      \n",
       "1565                  -0.113575                   1.979908    ...      \n",
       "1566                   0.651829                   2.432021    ...      \n",
       "1567                   0.536787                   2.921299    ...      \n",
       "1568                  -0.092935                   1.525592    ...      \n",
       "1569                   0.748059                   2.748431    ...      \n",
       "1570                   1.140674                   3.641949    ...      \n",
       "1571                   1.043265                   4.509830    ...      \n",
       "1572                   0.644737                   2.703706    ...      \n",
       "1573                   0.828856                   4.122412    ...      \n",
       "1574                   0.835241                   3.599408    ...      \n",
       "1575                   0.463498                   2.289197    ...      \n",
       "1576                   0.735594                   3.157729    ...      \n",
       "1577                   1.391975                   5.639768    ...      \n",
       "1578                   0.505777                   2.538045    ...      \n",
       "1579                   0.587836                   3.115611    ...      \n",
       "1580                   0.438240                   2.250966    ...      \n",
       "1581                   0.230096                   2.246752    ...      \n",
       "\n",
       "      shimmerLocal_sma_de_percentile99.0  shimmerLocal_sma_de_upleveltime75  \\\n",
       "0                               0.030707                           0.072727   \n",
       "1                               0.206826                           0.125000   \n",
       "2                               0.185000                           0.065217   \n",
       "3                               0.000000                           0.000000   \n",
       "4                               0.000000                           0.000000   \n",
       "5                               0.157520                           0.200000   \n",
       "6                               0.026929                           0.266667   \n",
       "7                               0.195774                           0.109589   \n",
       "8                               0.182839                           0.025907   \n",
       "9                               0.137699                           0.088235   \n",
       "10                              0.017937                           0.600000   \n",
       "11                              0.048705                           0.075758   \n",
       "12                              0.009998                           0.230769   \n",
       "13                              0.095232                           0.085714   \n",
       "14                              0.174237                           0.022059   \n",
       "15                              0.084998                           0.250000   \n",
       "16                              0.091322                           0.081633   \n",
       "17                              0.131123                           0.042254   \n",
       "18                              0.114048                           0.157303   \n",
       "19                              0.130152                           0.087500   \n",
       "20                              0.144395                           0.078947   \n",
       "21                              0.026227                           0.285714   \n",
       "22                              0.019787                           0.266667   \n",
       "23                              0.021571                           0.261905   \n",
       "24                              0.145755                           0.083333   \n",
       "25                              0.039671                           0.333333   \n",
       "26                              0.033469                           0.250000   \n",
       "27                              0.170124                           0.106383   \n",
       "28                              0.218582                           0.053571   \n",
       "29                              0.205201                           0.037037   \n",
       "...                                  ...                                ...   \n",
       "1552                            0.125382                           0.166667   \n",
       "1553                            0.236800                           0.111111   \n",
       "1554                            0.257890                           0.153846   \n",
       "1555                            0.165241                           0.200000   \n",
       "1556                            0.125597                           0.090909   \n",
       "1557                            0.108104                           0.092105   \n",
       "1558                            0.134254                           0.046875   \n",
       "1559                            0.162085                           0.015789   \n",
       "1560                            0.109535                           0.103448   \n",
       "1561                            0.021577                           0.190476   \n",
       "1562                            0.162363                           0.066116   \n",
       "1563                            0.165817                           0.161290   \n",
       "1564                            0.156104                           0.054422   \n",
       "1565                            0.032495                           0.500000   \n",
       "1566                            0.160502                           0.096154   \n",
       "1567                            0.164699                           0.016760   \n",
       "1568                            0.175008                           0.045455   \n",
       "1569                            0.040864                           0.131579   \n",
       "1570                            0.103205                           0.083333   \n",
       "1571                            0.266667                           0.070922   \n",
       "1572                            0.158598                           0.102273   \n",
       "1573                            0.104117                           0.040000   \n",
       "1574                            0.200632                           0.065421   \n",
       "1575                            0.075362                           0.087912   \n",
       "1576                            0.137919                           0.062500   \n",
       "1577                            0.068326                           0.153846   \n",
       "1578                            0.209278                           0.104575   \n",
       "1579                            0.205492                           0.035714   \n",
       "1580                            0.061972                           0.156250   \n",
       "1581                            0.187651                           0.075000   \n",
       "\n",
       "      shimmerLocal_sma_de_upleveltime90  F0final__Turn_numOnsets  \\\n",
       "0                              0.018182                      1.0   \n",
       "1                              0.050000                      3.0   \n",
       "2                              0.043478                      3.0   \n",
       "3                              0.000000                      0.0   \n",
       "4                              0.000000                      0.0   \n",
       "5                              0.066667                      3.0   \n",
       "6                              0.066667                      1.0   \n",
       "7                              0.041096                      6.0   \n",
       "8                              0.010363                     11.0   \n",
       "9                              0.049020                      8.0   \n",
       "10                             0.400000                      2.0   \n",
       "11                             0.030303                      1.0   \n",
       "12                             0.076923                      1.0   \n",
       "13                             0.028571                      3.0   \n",
       "14                             0.014706                      3.0   \n",
       "15                             0.125000                      2.0   \n",
       "16                             0.020408                      2.0   \n",
       "17                             0.014085                      2.0   \n",
       "18                             0.089888                      9.0   \n",
       "19                             0.050000                      5.0   \n",
       "20                             0.026316                      8.0   \n",
       "21                             0.285714                      1.0   \n",
       "22                             0.133333                      1.0   \n",
       "23                             0.047619                      1.0   \n",
       "24                             0.041667                      1.0   \n",
       "25                             0.190476                      2.0   \n",
       "26                             0.250000                      2.0   \n",
       "27                             0.021277                      3.0   \n",
       "28                             0.011905                     10.0   \n",
       "29                             0.006173                      5.0   \n",
       "...                                 ...                      ...   \n",
       "1552                           0.166667                      1.0   \n",
       "1553                           0.044444                      3.0   \n",
       "1554                           0.051282                      3.0   \n",
       "1555                           0.200000                      1.0   \n",
       "1556                           0.030303                      2.0   \n",
       "1557                           0.026316                      6.0   \n",
       "1558                           0.015625                      2.0   \n",
       "1559                           0.010526                      6.0   \n",
       "1560                           0.068966                      1.0   \n",
       "1561                           0.047619                      1.0   \n",
       "1562                           0.016529                      5.0   \n",
       "1563                           0.064516                      3.0   \n",
       "1564                           0.006803                      5.0   \n",
       "1565                           0.250000                      1.0   \n",
       "1566                           0.038462                      3.0   \n",
       "1567                           0.005587                      9.0   \n",
       "1568                           0.022727                      2.0   \n",
       "1569                           0.052632                      1.0   \n",
       "1570                           0.041667                      1.0   \n",
       "1571                           0.028369                     10.0   \n",
       "1572                           0.022727                      4.0   \n",
       "1573                           0.010000                      5.0   \n",
       "1574                           0.009346                      9.0   \n",
       "1575                           0.032967                      6.0   \n",
       "1576                           0.031250                      4.0   \n",
       "1577                           0.025641                      6.0   \n",
       "1578                           0.019608                      7.0   \n",
       "1579                           0.028571                      8.0   \n",
       "1580                           0.031250                      4.0   \n",
       "1581                           0.050000                      1.0   \n",
       "\n",
       "      F0final__Turn_duration   class       fid  uid  emotion      mean  \n",
       "0                       0.57  b'ACC'  01_MMK_2  013      SUR  2.166667  \n",
       "1                       0.62  b'ACC'    05_MYH  017      ACC  2.333333  \n",
       "2                       0.55  b'FEA'  01_MAD_1  396      JOY  2.055556  \n",
       "3                       0.16  b'SUR'  01_MMK_1  430      SUR  1.888889  \n",
       "4                       0.29  b'SAD'  01_MMK_1  397      SAD  3.111111  \n",
       "5                       0.19  b'ANG'  01_MAD_1  081      ANG  2.444444  \n",
       "6                       0.19  b'SUR'    04_MNN  703      SUR  1.777778  \n",
       "7                       0.87  b'ANT'    02_MTN  042      ANT  3.166667  \n",
       "8                       3.00  b'ANT'  01_MAD_1  569      ANT  2.722222  \n",
       "9                       1.43  b'ANT'  01_MMK_1  109      JOY  2.500000  \n",
       "10                      0.07  b'SUR'    02_MTN  093      SUR  2.777778  \n",
       "11                      0.68  b'ACC'    02_MTN  355      ACC  2.611111  \n",
       "12                      0.15  b'DIS'    02_MTN  880      DIS  2.777778  \n",
       "13                      1.08  b'DIS'    02_MTN  507      DIS  3.111111  \n",
       "14                      1.43  b'ANT'    03_FTY  378      ANT  3.444444  \n",
       "15                      0.10  b'SUR'    02_MTN  166      SUR  3.666667  \n",
       "16                      0.74  b'ACC'    05_MYH  224      ACC  2.666667  \n",
       "17                      0.76  b'SAD'    03_FTY  220      SAD  3.611111  \n",
       "18                      1.11  b'OTH'  01_MMK_1  603      ANT  3.222222  \n",
       "19                      0.99  b'ANT'    02_MTN  799      SUR  2.333333  \n",
       "20                      1.96  b'NEU'    02_MTN  748      ANT  3.222222  \n",
       "21                      0.17  b'DIS'  01_MAD_1  515      DIS  3.111111  \n",
       "22                      0.17  b'SUR'    04_MNN  530      SUR  1.555556  \n",
       "23                      0.44  b'SAD'    03_FTY  137      SAD  2.944444  \n",
       "24                      0.63  b'ANG'  01_MAD_2  136      ANG  2.611111  \n",
       "25                      0.30  b'SUR'    02_MTN  329      SUR  2.777778  \n",
       "26                      0.17  b'SUR'  01_MAD_1  174      DIS  2.944444  \n",
       "27                      0.67  b'ACC'  01_MMK_2  068      ACC  3.277778  \n",
       "28                      2.19  b'DIS'    02_MTN  617      DIS  3.277778  \n",
       "29                      1.84  b'FEA'    02_MTN  161      FEA  3.777778  \n",
       "...                      ...     ...       ...  ...      ...       ...  \n",
       "1552                    0.14  b'ANG'  01_MAD_1  284      DIS  2.777778  \n",
       "1553                    0.73  b'FEA'    03_FTY  560      FEA  3.277778  \n",
       "1554                    0.41  b'FEA'  01_MMK_1  522      FEA  3.500000  \n",
       "1555                    0.22  b'SUR'  01_MMK_1  254      SUR  3.055556  \n",
       "1556                    0.51  b'ACC'    04_MNN  611      ACC  2.222222  \n",
       "1557                    1.14  b'ANG'  01_MAD_1  193      ANG  2.888889  \n",
       "1558                    0.73  b'FEA'    03_FTY  216      FEA  2.777778  \n",
       "1559                    2.85  b'ANT'    04_MNN  488      ANT  3.055556  \n",
       "1560                    0.33  b'SUR'    02_MTN  186      FEA  2.833333  \n",
       "1561                    0.23  b'ACC'    02_MTN  240      ACC  3.055556  \n",
       "1562                    1.40  b'ANG'    05_MYH  102      ANG  2.944444  \n",
       "1563                    0.42  b'ANT'    02_MTN  779      SUR  2.555556  \n",
       "1564                    1.59  b'DIS'    03_FTY  106      SAD  2.333333  \n",
       "1565                    0.06  b'SUR'  01_MMK_1  282      SUR  2.611111  \n",
       "1566                    0.69  b'ANT'  01_MAD_2  107      ANT  2.111111  \n",
       "1567                    2.16  b'JOY'  01_MMK_1  525      JOY  4.055556  \n",
       "1568                    0.61  b'SAD'    03_FTY  211      SAD  2.777778  \n",
       "1569                    0.74  b'JOY'  01_MMK_1  377      JOY  2.277778  \n",
       "1570                    0.49  b'SAD'    04_MNN  095      SAD  2.388889  \n",
       "1571                    1.56  b'SAD'    04_MNN  732      SAD  2.055556  \n",
       "1572                    1.15  b'ACC'    04_MNN  353      JOY  2.611111  \n",
       "1573                    1.02  b'ANT'    03_FTY  022      JOY  3.111111  \n",
       "1574                    2.00  b'ACC'  01_MMK_1  138      ACC  2.888889  \n",
       "1575                    0.98  b'SUR'    02_MTN  705      SUR  3.555556  \n",
       "1576                    0.79  b'JOY'  01_MMK_1  559      JOY  3.333333  \n",
       "1577                    1.56  b'SAD'  01_MAD_1  139      SAD  2.500000  \n",
       "1578                    1.74  b'ANG'    05_MYH  269      ANG  2.777778  \n",
       "1579                    1.74  b'OTH'  01_MMK_1  588      JOY  2.944444  \n",
       "1580                    0.85  b'ANG'    02_MTN  412      ANG  2.777778  \n",
       "1581                    0.61  b'ANT'    02_MTN  860      SUR  3.000000  \n",
       "\n",
       "[1582 rows x 1587 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: Regression per emotion\n",
    "==\n",
    "\n",
    "For each emotion label (as given by instances.txt), run a regression on the average intensity rating. Find a \"best\" configuration first, by trying different feature selections on a train set and use this configuration to predict on a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['SUR', 'DIS', 'ANT', 'ACC', 'JOY', 'SAD', 'ANG', 'FEA']\n",
    "best = {}\n",
    "for label in labels:\n",
    "    # get features and labels, split into dataframe\n",
    "    best[label] = [float('inf')]\n",
    "    df_sub = df.loc[df['emotion'] == label]\n",
    "    df_sub = df_sub.drop(['class', 'fid', 'uid', 'emotion'], axis=1)\n",
    "    df_train, df_test = np.array_split(df_sub, [int(len(df_sub) * 0.8)])\n",
    "    features, labels = np.split(df_train.as_matrix(), [-1], axis=1)\n",
    "    labels = [label[0] for label in labels]\n",
    "    std = np.std(labels)\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \n",
    "    # try feature counts and selection methods on train to find \"best\"\n",
    "    for k in ['all'] + [int(len(df_train) / d) \n",
    "                        for d in [pow(2, i) for i in range(-2, 6)]]:\n",
    "        for reg in [fs.f_regression, fs.mutual_info_regression]:\n",
    "            sel = fs.SelectKBest(reg, k=k).fit(features, labels)\n",
    "            # this shows which features were selected:\n",
    "            # df_train.axes[1][:-1][sel.get_support()]\n",
    "            features_new = sel.transform(features)\n",
    "            mod = linear_model.LinearRegression(n_jobs=-1)\n",
    "            predicted = cross_val_predict(mod, features_new, \n",
    "                                          labels, n_jobs=-1)\n",
    "            mse = metrics.mean_squared_error(labels, predicted)\n",
    "            mae = metrics.mean_absolute_error(labels, predicted)\n",
    "            if mse < best[label][0]:\n",
    "                best[label] = [mse, mae, std, reg, k]\n",
    "    \n",
    "    # train with \"best\" config and predict on test\n",
    "    sel = fs.SelectKBest(best[label][3], k=best[label][4]\n",
    "                     ).fit(features, labels)\n",
    "    features_new = sel.transform(features)\n",
    "    mod = linear_model.LinearRegression(n_jobs=-1\n",
    "                                       ).fit(features_new, labels)\n",
    "    features_test, labels_test = np.split(df_test.as_matrix(), \n",
    "                                          [-1], axis=1)\n",
    "    features_test_new = sel.transform(features_test)\n",
    "    predicted = mod.predict(features_test_new)\n",
    "    mse = metrics.mean_squared_error(labels_test, predicted)\n",
    "    mae = metrics.mean_absolute_error(labels_test, predicted)\n",
    "    mean = sum(labels) / len(labels)\n",
    "    predicted_bl = [mean for label in labels_test]\n",
    "    mse_bl = metrics.mean_squared_error(labels_test, predicted_bl)\n",
    "    mae_bl = metrics.mean_absolute_error(labels_test, predicted_bl)\n",
    "    best[label].extend([mse, mae, np.std(labels_test), len(df_train),\n",
    "                        mse_bl, mae_bl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a) label</th>\n",
       "      <th>b) feature selection function</th>\n",
       "      <th>c) #features</th>\n",
       "      <th>d) mse</th>\n",
       "      <th>e) mse (bl)</th>\n",
       "      <th>f) mae</th>\n",
       "      <th>g) mae (bl)</th>\n",
       "      <th>h) std on test</th>\n",
       "      <th>i) training size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANG</td>\n",
       "      <td>f_regression</td>\n",
       "      <td>3</td>\n",
       "      <td>0.166417</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.329819</td>\n",
       "      <td>0.302490</td>\n",
       "      <td>0.364062</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FEA</td>\n",
       "      <td>f_regression</td>\n",
       "      <td>2</td>\n",
       "      <td>0.160192</td>\n",
       "      <td>0.133973</td>\n",
       "      <td>0.296046</td>\n",
       "      <td>0.295357</td>\n",
       "      <td>0.364420</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JOY</td>\n",
       "      <td>mutual_info_regression</td>\n",
       "      <td>10</td>\n",
       "      <td>0.239744</td>\n",
       "      <td>0.304603</td>\n",
       "      <td>0.401909</td>\n",
       "      <td>0.445571</td>\n",
       "      <td>0.521202</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACC</td>\n",
       "      <td>mutual_info_regression</td>\n",
       "      <td>4</td>\n",
       "      <td>0.116489</td>\n",
       "      <td>0.163575</td>\n",
       "      <td>0.290901</td>\n",
       "      <td>0.346082</td>\n",
       "      <td>0.402424</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAD</td>\n",
       "      <td>mutual_info_regression</td>\n",
       "      <td>4</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>0.205810</td>\n",
       "      <td>0.374664</td>\n",
       "      <td>0.395233</td>\n",
       "      <td>0.433426</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DIS</td>\n",
       "      <td>f_regression</td>\n",
       "      <td>5</td>\n",
       "      <td>0.248469</td>\n",
       "      <td>0.205426</td>\n",
       "      <td>0.392227</td>\n",
       "      <td>0.359169</td>\n",
       "      <td>0.451714</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ANT</td>\n",
       "      <td>f_regression</td>\n",
       "      <td>4</td>\n",
       "      <td>0.156105</td>\n",
       "      <td>0.196865</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>0.349237</td>\n",
       "      <td>0.435063</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SUR</td>\n",
       "      <td>mutual_info_regression</td>\n",
       "      <td>38</td>\n",
       "      <td>0.234912</td>\n",
       "      <td>0.470990</td>\n",
       "      <td>0.346335</td>\n",
       "      <td>0.516507</td>\n",
       "      <td>0.686273</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  a) label b) feature selection function  c) #features    d) mse  e) mse (bl)  \\\n",
       "0      ANG                  f_regression             3  0.166417     0.140072   \n",
       "1      FEA                  f_regression             2  0.160192     0.133973   \n",
       "2      JOY        mutual_info_regression            10  0.239744     0.304603   \n",
       "3      ACC        mutual_info_regression             4  0.116489     0.163575   \n",
       "4      SAD        mutual_info_regression             4  0.191111     0.205810   \n",
       "5      DIS                  f_regression             5  0.248469     0.205426   \n",
       "6      ANT                  f_regression             4  0.156105     0.196865   \n",
       "7      SUR        mutual_info_regression            38  0.234912     0.470990   \n",
       "\n",
       "     f) mae  g) mae (bl)  h) std on test  i) training size  \n",
       "0  0.329819     0.302490        0.364062               116  \n",
       "1  0.296046     0.295357        0.364420                69  \n",
       "2  0.401909     0.445571        0.521202               172  \n",
       "3  0.290901     0.346082        0.402424               131  \n",
       "4  0.374664     0.395233        0.433426               136  \n",
       "5  0.392227     0.359169        0.451714               178  \n",
       "6  0.301512     0.349237        0.435063               152  \n",
       "7  0.346335     0.516507        0.686273               308  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(\n",
    "    {\n",
    "     'a) label':\n",
    "         [key for key, val in best.items()],\n",
    "     'b) feature selection function':\n",
    "         [str(val[3]).split()[1] for key, val in best.items()],\n",
    "     'c) #features':\n",
    "         [val[4] for key, val in best.items()],\n",
    "     'd) mse':\n",
    "         [val[5] for key, val in best.items()],\n",
    "     'e) mse (bl)':\n",
    "         [val[9] for key, val in best.items()],\n",
    "     'f) mae':\n",
    "         [val[6] for key, val in best.items()],\n",
    "     'g) mae (bl)':\n",
    "         [val[10] for key, val in best.items()],\n",
    "     'h) std on test':\n",
    "         [val[7] for key, val in best.items()],\n",
    "     'i) training size':\n",
    "         [val[8] for key, val in best.items()],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEWCAYAAADCeVhIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nubzjyck551ze+f/S9h1V4YLrzQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fddc2b9a908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [val[8] for key, val in best.items()]\n",
    "y = [val[6] / val[7] for key, val in best.items()]\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "bl = [val[10] / val[7] for key, val in best.items()]\n",
    "plt.plot(x, y, 'x', x, p(x), 'r--', x, bl, 'g.')\n",
    "plt.xlabel('training data size')\n",
    "plt.ylabel('mae in standard deviations')\n",
    "plt.title('Effect of training size on mae & comparison with baseline (green)')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion\n",
    "==\n",
    "\n",
    "The experiment does not reveal a clear preference for either feature selection method, with each being found to perform better than the other four times. This could mean that the data mostly contains linear dependencies between features, which both methods can detect, and not more complex dependencies, which only the mutual information method can detect. Alternatively, it could be that the amount of data for most emotions is not large enough for the mutual information method to work effectively. Perhaps that is why for SUR, with the largest training set, a much greater number of features was selected (using mutual information) and a large improvement over the baseline (mean rating observed during training) was achieved. That is, perhaps only for this emotion was there enough data to detect more than linear dependencies, allowing for a better selection of features besides obviously providing more data to learn from.\n",
    "\n",
    "As expected, there is a clear tendency for a larger training set to yield a lower mean absolute error (normalized by standard deviation of ratings per emotion). For most of the emotions an improvement over the baseline was achieved. However, for ANG and DIS there was a notable decrease in the prediction quality over the baseline, of similar magnitude as the improvements for the emotions of comparable training size. This might mean that training sizes below 200 are too small and the improvements there are not significant, whereas the improvement for SUR certainly seems significant. I did not perform any analysis of significance to verify this, though.\n",
    "\n",
    "In conclusion, with \"enough\" data it is possible to predict emotion intensity ratings from the given features within as little as 0.5 standard deviations, a notable improvement over the baseline. Smaller amounts of data seem insufficient to improve on the baseline with the methods tested here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2: Regression over all emotions\n",
    "==\n",
    "\n",
    "Run regression on the mean intensity rating for all emotions at once. Find a \"best\" feature selection configuration on a train set first, then use this to predict on a test set.\n",
    "\n",
    "The code is largely the same as for experiment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22148241238425265, 0.3775465314949496, 0.53665305986651179, <function mutual_info_regression at 0x7fddc4b4cbf8>, 9, 0.2141868544412846, 0.36618936078105419, 0.54022814726838286, 1265, 0.29475775689070416, 0.4236895078892064]\n"
     ]
    }
   ],
   "source": [
    "# get features and labels, split into dataframe\n",
    "df_cp = df\n",
    "df_cp = df.drop(['class', 'fid', 'uid', 'emotion'], axis=1)\n",
    "df_train, df_test = np.array_split(df_cp, [int(len(df_cp) * 0.8)])\n",
    "features, labels = np.split(df_train.as_matrix(), [-1], axis=1)\n",
    "labels = [label[0] for label in labels]\n",
    "std = np.std(labels)\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "# try feature counts and selection methods on train to find \"best\"\n",
    "best = [float('inf')]\n",
    "for k in [int(len(df_train) / d) \n",
    "          for d in [pow(2, i) for i in range(0, 11)]]:\n",
    "    for reg in [fs.f_regression, fs.mutual_info_regression]:\n",
    "        sel = fs.SelectKBest(reg, k=k).fit(features, labels)\n",
    "        # this shows which features were selected:\n",
    "        # df_train.axes[1][:-1][sel.get_support()]\n",
    "        features_new = sel.transform(features)\n",
    "        mod = linear_model.LinearRegression(n_jobs=-1)\n",
    "        predicted = cross_val_predict(mod, features_new, \n",
    "                                      labels, n_jobs=-1)\n",
    "        mse = metrics.mean_squared_error(labels, predicted)\n",
    "        mae = metrics.mean_absolute_error(labels, predicted)\n",
    "        if mse < best[0]:\n",
    "            best = [mse, mae, std, reg, k]\n",
    "\n",
    "# train with \"best\" config and predict on test\n",
    "sel = fs.SelectKBest(best[3], k=best[4]).fit(features, labels)\n",
    "features_new = sel.transform(features)\n",
    "mod = linear_model.LinearRegression(n_jobs=-1\n",
    "                                   ).fit(features_new, labels)\n",
    "features_test, labels_test = np.split(df_test.as_matrix(), \n",
    "                                      [-1], axis=1)\n",
    "features_test_new = sel.transform(features_test)\n",
    "predicted = mod.predict(features_test_new)\n",
    "mse = metrics.mean_squared_error(labels_test, predicted)\n",
    "mae = metrics.mean_absolute_error(labels_test, predicted)\n",
    "mean = sum(labels) / len(labels)\n",
    "predicted_bl = [mean for label in labels_test]\n",
    "mse_bl = metrics.mean_squared_error(labels_test, predicted_bl)\n",
    "mae_bl = metrics.mean_absolute_error(labels_test, predicted_bl)\n",
    "\n",
    "best.extend([mse, mae, np.std(labels_test), len(df_train), mse_bl, mae_bl])\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion\n",
    "==\n",
    "\n",
    "Prediction of intensity for all emotions at once was not very successful. With 0.68 standard deviations mean absolute error, the improvement over the baseline of 0.78 is not very large, and the result is comparable to those achieved for much smaller training sizes for individual emotions. Thus, it appears that intensity is expressed differently across emotions and the model was not able to capture that fully with the training data given.\n",
    "\n",
    "Suprisingly, selection of only 9 features yielded the best results in cross-validation on the training set. One might have expected a larger number to perform better. A training size of 1265 should be enough to make use of 18 features, at least, which would have been the next higher number the algorithm attempted. I cannot think of a good explanation for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 3: Classification with training on \"high intensity\" instances\n",
    "==\n",
    "\n",
    "Train a classifier only on instances with a \"high\" rating for \"intensity\". Try different thresholds and for each one find a \"best\" configuration for feature selection on a train set, then evaluate on a dev set (including low intensity instances). Use the \"best\" threshold and its \"best\" feature selection configuration to learn on the train set and predict the test set (including low intensity instances).\n",
    "\n",
    "The idea here is that training only on \"high\" intensity instances might allow the model to better pick up on stereotypical characteristics of each emotional expression, making for a clearer distinction between the emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.34035827186512119, <function mutual_info_classif at 0x7fd7b392ac80>, 29, 0.33544303797468356]\n",
      "[1.8, 0.34159482758620691, <function mutual_info_classif at 0x7fd7b392ac80>, 232, 0.31329113924050633]\n",
      "[1.9, 0.3493975903614458, <function mutual_info_classif at 0x7fd7b392ac80>, 28, 0.33860759493670883]\n",
      "[2.0, 0.35083798882681566, <function mutual_info_classif at 0x7fd7b392ac80>, 55, 0.30379746835443039]\n",
      "[2.1, 0.35487528344671204, <function mutual_info_classif at 0x7fd7b392ac80>, 27, 0.33860759493670883]\n",
      "[2.2, 0.35112692763938314, <function f_classif at 0x7fd7b498d6a8>, 52, 0.32911392405063289]\n",
      "[2.3, 0.35308953341740229, <function mutual_info_classif at 0x7fd7b392ac80>, 24, 0.32911392405063289]\n",
      "[2.4, 0.35753424657534244, <function mutual_info_classif at 0x7fd7b392ac80>, 365, 0.27848101265822783]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5, 0.34550989345509892, <function f_classif at 0x7fd7b498d6a8>, 20, 0.27848101265822783]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6, 0.36142625607779577, <function mutual_info_classif at 0x7fd7b392ac80>, 308, 0.28797468354430378]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7, 0.35873605947955389, <function mutual_info_classif at 0x7fd7b392ac80>, 269, 0.24367088607594936]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.8, 0.35714285714285715, <function f_classif at 0x7fd7b498d6a8>, 224, 0.26582278481012656]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9, 0.39361702127659576, <function mutual_info_classif at 0x7fd7b392ac80>, 47, 0.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [673] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 0.37627118644067797, <function mutual_info_classif at 0x7fd7b392ac80>, 73, 0.21518987341772153]\n"
     ]
    }
   ],
   "source": [
    "df_cp = df.drop(['class', 'fid', 'uid'], axis=1)\n",
    "df_train, df_dev, df_test = np.array_split(\n",
    "    df_cp, [int(len(df_cp) * 0.6), int(len(df_cp) * 0.8)])\n",
    "\n",
    "best = {}\n",
    "best['all'] = [float('-inf')]\n",
    "\n",
    "for thresh in [0.0] + [i/10.0 for i in range(18, 31)]:\n",
    "    # reduce train set to instances above threshold\n",
    "    hi = df_train['mean'] > thresh\n",
    "    df_train_new = df_train.loc[hi]\n",
    "    df_train_new = df_train_new.drop('mean', axis=1)\n",
    "    features, labels = np.split(\n",
    "        df_train_new.as_matrix(), [-1], axis=1)\n",
    "    labels = [label[0] for label in labels]\n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \n",
    "    # try feature counts and selection methods on train to find \"best\"\n",
    "    # configuration for this thresh\n",
    "    best[thresh] = [float('-inf')]\n",
    "    for k in [int(len(df_train_new) / d) \n",
    "              for d in [pow(2, i) for i in range(1, 8)]]:\n",
    "        for reg in [fs.f_classif\n",
    "                   ,fs.mutual_info_classif\n",
    "                   ]:\n",
    "            sel = fs.SelectKBest(reg, k=k).fit(features, labels)\n",
    "            # this shows which features were selected:\n",
    "            # df_train.axes[1][:-1][sel.get_support()]\n",
    "            features_new = sel.transform(features)\n",
    "            wclf = svm.SVC(kernel='linear', class_weight='balanced')\n",
    "            predicted = cross_val_predict(wclf, features_new, \n",
    "                                          labels, n_jobs=-1)\n",
    "            acc = metrics.accuracy_score(labels, predicted)\n",
    "            # print(k, reg, acc)\n",
    "            if acc > best[thresh][0]:\n",
    "                best[thresh] = [acc, reg, k]\n",
    "\n",
    "    # train with \"best\" config for this thresh and predict on dev\n",
    "    sel = fs.SelectKBest(best[thresh][1], k=best[thresh][2]\n",
    "                        ).fit(features, labels)\n",
    "    features_new = sel.transform(features)\n",
    "    wclf = svm.SVC(kernel='linear', class_weight='balanced'\n",
    "                  ).fit(features_new, labels)\n",
    "    df_dev_new = df_dev.drop('mean', axis=1)\n",
    "    features_dev, labels_dev = np.split(df_dev_new.as_matrix(), \n",
    "                                        [-1], axis=1)\n",
    "    features_dev_new = sel.transform(features_dev)\n",
    "    predicted = wclf.predict(features_dev_new)\n",
    "    acc = metrics.accuracy_score(labels_dev, predicted)\n",
    "    best[thresh].extend([acc])\n",
    "    print([thresh] + best[thresh])\n",
    "    if acc > best['all'][0]:\n",
    "        best['all'] = [thresh] + best[thresh]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.309148264984\n",
      "0.353312302839\n"
     ]
    }
   ],
   "source": [
    "# train with \"best\" thresh and its \"best\" config on train \n",
    "# and predict on test\n",
    "df_train_new = df_train\n",
    "hi = df_train_new['mean'] > best['all'][0]\n",
    "df_train_new = df_train_new.loc[hi]\n",
    "df_train_new = df_train_new.drop('mean', axis=1)\n",
    "features, labels = np.split(df_train_new.as_matrix(), [-1], axis=1)\n",
    "labels = [label[0] for label in labels]\n",
    "\n",
    "sel = fs.SelectKBest(best['all'][2], k=best['all'][3]\n",
    "                    ).fit(features, labels)\n",
    "features_new = sel.transform(features)\n",
    "wclf = svm.SVC(kernel='linear', class_weight='balanced'\n",
    "              ).fit(features_new, labels)\n",
    "\n",
    "df_test_new = df_test.drop('mean', axis=1)\n",
    "features_test, labels_test = np.split(df_test_new.as_matrix(), \n",
    "                                      [-1], axis=1)\n",
    "labels_test = [label[0] for label in labels_test]\n",
    "\n",
    "features_test_new = sel.transform(features_test)\n",
    "predicted = wclf.predict(features_test_new)\n",
    "acc = metrics.accuracy_score(labels_test, predicted)\n",
    "print(acc)\n",
    "\n",
    "# train with 0.0 thresh and its \"best\" config on train\n",
    "# and predict on test\n",
    "df_train_new = df_train\n",
    "df_train_new = df_train_new.drop('mean', axis=1)\n",
    "features, labels = np.split(df_train_new.as_matrix(), [-1], axis=1)\n",
    "labels = [label[0] for label in labels]\n",
    "\n",
    "sel = fs.SelectKBest(best[0.0][1], k=best[0.0][2]\n",
    "                    ).fit(features, labels)\n",
    "features_new = sel.transform(features)\n",
    "wclf = svm.SVC(kernel='linear', class_weight='balanced'\n",
    "              ).fit(features_new, labels)\n",
    "\n",
    "df_test_new = df_test.drop('mean', axis=1)\n",
    "features_test, labels_test = np.split(df_test_new.as_matrix(), \n",
    "                                      [-1], axis=1)\n",
    "labels_test = [label[0] for label in labels_test]\n",
    "\n",
    "features_test_new = sel.transform(features_test)\n",
    "predicted = wclf.predict(features_test_new)\n",
    "acc = metrics.accuracy_score(labels_test, predicted)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.271293375394 0.125\n"
     ]
    }
   ],
   "source": [
    "# rough baseline -- majority class\n",
    "print(max(df_test['emotion'].value_counts())/len(df_test['emotion']), 1/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Discussion\n",
    "==\n",
    "\n",
    "The hypothesis has proven to be false. The model that uses all training instances (intensity threshold of 0.0) outperforms the model using the \"best\" threshold for intensity by 4.4 percentage points. \n",
    "A comparison of the accuracies for different thresholds on the train and dev sets seems to explain this. While accuracy on the train set improves from 0.34 for threshold 0.0 to above 0.39 for threshold 2.9, the accuracy on the dev set for those same thresholds goes down from 0.335 to 0.25, respectively. So while having to classify only high-intensity instances does make classification easier, the model learned from those does not generalize to low intensity instances and achieves poor accuracy for them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
